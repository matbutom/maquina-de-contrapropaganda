{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/matbutom/maquina-de-contrapropaganda/blob/main/copy_of_tensorflow_datasets.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6XvCUmCEd4Dm"
      },
      "source": [
        "# TensorFlow Datasets\n",
        "\n",
        "TFDS provides a collection of ready-to-use datasets for use with TensorFlow, Jax, and other Machine Learning frameworks.\n",
        "\n",
        "It handles downloading and preparing the data deterministically and constructing a `tf.data.Dataset` (or `np.array`).\n",
        "\n",
        "Note: Do not confuse [TFDS](https://www.tensorflow.org/datasets) (this library) with `tf.data` (TensorFlow API to build efficient data pipelines). TFDS is a high level wrapper around `tf.data`. If you're not familiar with this API, we encourage you to read [the official tf.data guide](https://www.tensorflow.org/guide/data) first.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J8y9ZkLXmAZc"
      },
      "source": [
        "Copyright 2018 The TensorFlow Datasets Authors, Licensed under the Apache License, Version 2.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OGw9EgE0tC0C"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://www.tensorflow.org/datasets/overview\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\" />View on TensorFlow.org</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/datasets/blob/master/docs/overview.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://github.com/tensorflow/datasets/blob/master/docs/overview.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a href=\"https://storage.googleapis.com/tensorflow_docs/datasets/docs/overview.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\" />Download notebook</a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_7hshda5eaGL"
      },
      "source": [
        "## Installation\n",
        "\n",
        "TFDS exists in two packages:\n",
        "\n",
        "* `pip install tensorflow-datasets`: The stable version, released every few months.\n",
        "* `pip install tfds-nightly`: Released every day, contains the last versions of the datasets.\n",
        "\n",
        "This colab uses `tfds-nightly`:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "both",
        "id": "boeZp0sYbO41"
      },
      "outputs": [],
      "source": [
        "#!pip install tfds-nightly tensorflow matplotlib apache_beam mlcroissant"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TTBSvHcSLBzc"
      },
      "outputs": [],
      "source": [
        "#import matplotlib.pyplot as plt\n",
        "#import numpy as np\n",
        "#import tensorflow as tf\n",
        "\n",
        "#import tensorflow_datasets as tfds"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VZZyuO13fPvk"
      },
      "source": [
        "## Find available datasets\n",
        "\n",
        "All dataset builders are subclass of `tfds.core.DatasetBuilder`. To get the list of available builders, use `tfds.list_builders()` or look at our [catalog](https://www.tensorflow.org/datasets/catalog/overview)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FAvbSVzjLCIb"
      },
      "outputs": [],
      "source": [
        "#tfds.list_builders()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VjI6VgOBf0v0"
      },
      "source": [
        "## Load a dataset\n",
        "\n",
        "### tfds.load\n",
        "\n",
        "The easiest way of loading a dataset is `tfds.load`. It will:\n",
        "\n",
        "1. Download the data and save it as [`tfrecord`](https://www.tensorflow.org/tutorials/load_data/tfrecord) files.\n",
        "2. Load the `tfrecord` and create the `tf.data.Dataset`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dCou80mnLLPV"
      },
      "outputs": [],
      "source": [
        "#ds = tfds.load('mnist', split='train', shuffle_files=True)\n",
        "#assert isinstance(ds, tf.data.Dataset)\n",
        "#print(ds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "byOXYCEJS7S6"
      },
      "source": [
        "Some common arguments:\n",
        "\n",
        "*   `split=`: Which split to read (e.g. `'train'`, `['train', 'test']`, `'train[80%:]'`,...). See our [split API guide](https://www.tensorflow.org/datasets/splits).\n",
        "*   `shuffle_files=`: Control whether to shuffle the files between each epoch (TFDS store big datasets in multiple smaller files).\n",
        "*   `data_dir=`: Location where the dataset is saved (\n",
        "defaults to `~/tensorflow_datasets/`)\n",
        "*   `with_info=True`: Returns the `tfds.core.DatasetInfo` containing dataset metadata\n",
        "*   `download=False`: Disable download\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qeNmFx_1RXCb"
      },
      "source": [
        "### tfds.builder\n",
        "\n",
        "`tfds.load` is a thin wrapper around `tfds.core.DatasetBuilder`. You can get the same output using the `tfds.core.DatasetBuilder` API:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2zN_jQ2ER40W"
      },
      "outputs": [],
      "source": [
        "#builder = tfds.builder('mnist')\n",
        "# 1. Create the tfrecord files (no-op if already exists)\n",
        "#builder.download_and_prepare()\n",
        "# 2. Load the `tf.data.Dataset`\n",
        "#ds = builder.as_dataset(split='train', shuffle_files=True)\n",
        "#print(ds)\n",
        "#"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IwrjccfjoQCD"
      },
      "source": [
        "### `tfds build` CLI\n",
        "\n",
        "If you want to generate a specific dataset, you can use the [`tfds` command line](https://www.tensorflow.org/datasets/cli). For example:\n",
        "\n",
        "```sh\n",
        "tfds build mnist\n",
        "```\n",
        "\n",
        "See [the doc](https://www.tensorflow.org/datasets/cli) for available flags."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!tfds new datos"
      ],
      "metadata": {
        "id": "3bVkfPUUx630"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd datos/\n",
        "%pwd"
      ],
      "metadata": {
        "id": "7RsQ5fADzEMy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "id": "n1oBK0o4zH2P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aW132I-rbJXE"
      },
      "source": [
        "## Iterate over a dataset\n",
        "\n",
        "### As dict\n",
        "\n",
        "By default, the `tf.data.Dataset` object contains a `dict` of `tf.Tensor`s:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JAGjXdk_bIYQ"
      },
      "outputs": [],
      "source": [
        "# ds = tfds.load('mnist', split='train')\n",
        "# ds = ds.take(1)  # Only take a single example\n",
        "\n",
        "# for example in ds:  # example is `{'image': tf.Tensor, 'label': tf.Tensor}`\n",
        "#  print(list(example.keys()))\n",
        "#  image = example[\"image\"]\n",
        "#  label = example[\"label\"]\n",
        "#  print(image.shape, label)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sIqX2bmhu-8d"
      },
      "source": [
        "To find out the `dict` key names and structure, look at the dataset documentation in [our catalog](https://www.tensorflow.org/datasets/catalog/overview#all_datasets). For example: [mnist documentation](https://www.tensorflow.org/datasets/catalog/mnist)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "umAtqBBqdkDG"
      },
      "source": [
        "### As tuple (`as_supervised=True`)\n",
        "\n",
        "By using `as_supervised=True`, you can get a tuple `(features, label)` instead for supervised datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nJ4O0xy3djfV"
      },
      "outputs": [],
      "source": [
        "# ds = tfds.load('mnist', split='train', as_supervised=True)\n",
        "# ds = ds.take(1)\n",
        "\n",
        "# for image, label in ds:  # example is (image, label)\n",
        "#  print(image.shape, label)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u9palgyHfEwQ"
      },
      "source": [
        "### As numpy (`tfds.as_numpy`)\n",
        "\n",
        "Uses `tfds.as_numpy` to convert:\n",
        "\n",
        "*   `tf.Tensor` -> `np.array`\n",
        "*   `tf.data.Dataset` -> `Iterator[Tree[np.array]]` (`Tree` can be arbitrary nested `Dict`, `Tuple`)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tzQTCUkAfe9R"
      },
      "outputs": [],
      "source": [
        "# ds = tfds.load('mnist', split='train', as_supervised=True)\n",
        "# ds = ds.take(1)\n",
        "\n",
        "# for image, label in tfds.as_numpy(ds):\n",
        " # print(type(image), type(label), label)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XaRN-LdXUkl_"
      },
      "source": [
        "### As batched tf.Tensor (`batch_size=-1`)\n",
        "\n",
        "By using `batch_size=-1`, you can load the full dataset in a single batch.\n",
        "\n",
        "This can be combined with `as_supervised=True` and `tfds.as_numpy` to get the the data as `(np.array, np.array)`:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gg8BNsv-UzFl"
      },
      "outputs": [],
      "source": [
        "# image, label = tfds.as_numpy(tfds.load(\n",
        "#    'mnist',\n",
        "#   split='test',\n",
        "#    batch_size=-1,\n",
        "#    as_supervised=True,\n",
        "# ))\n",
        "\n",
        "# print(type(image), image.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kRJrB3L6wgKI"
      },
      "source": [
        "Be careful that your dataset can fit in memory, and that all examples have the same shape."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "heaKNg7-X4jN"
      },
      "source": [
        "## Benchmark your datasets\n",
        "\n",
        "Benchmarking a dataset is a simple `tfds.benchmark` call on any iterable (e.g. `tf.data.Dataset`, `tfds.as_numpy`,...).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZyQzZ98bX3dM"
      },
      "outputs": [],
      "source": [
        "# ds = tfds.load('mnist', split='train')\n",
        "# ds = ds.batch(32).prefetch(1)\n",
        "\n",
        "# tfds.benchmark(ds, batch_size=32)\n",
        "# tfds.benchmark(ds, batch_size=32)  # Second epoch much faster due to auto-caching"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MT0yEX_4kYnV"
      },
      "source": [
        "* Do not forget to normalize the results per batch size with the `batch_size=` kwarg.\n",
        "* In the summary, the first warmup batch is separated from the other ones to capture `tf.data.Dataset` extra setup time (e.g. buffers initialization,...).\n",
        "* Notice how the second iteration is much faster due to [TFDS auto-caching](https://www.tensorflow.org/datasets/performances#auto-caching).\n",
        "* `tfds.benchmark` returns a `tfds.core.BenchmarkResult` which can be inspected for further analysis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o-cuwvVbeb43"
      },
      "source": [
        "### Build end-to-end pipeline\n",
        "\n",
        "To go further, you can look:\n",
        "\n",
        "*   Our [end-to-end Keras example](https://www.tensorflow.org/datasets/keras_example) to see a full training pipeline (with batching, shuffling,...).\n",
        "*   Our [performance guide](https://www.tensorflow.org/datasets/performances) to improve the speed of your pipelines (tip: use `tfds.benchmark(ds)` to benchmark your datasets).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gTRTEQqscxAE"
      },
      "source": [
        "## Visualization\n",
        "\n",
        "### tfds.as_dataframe\n",
        "\n",
        "`tf.data.Dataset` objects can be converted to [`pandas.DataFrame`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html) with `tfds.as_dataframe` to be visualized on [Colab](https://colab.research.google.com).\n",
        "\n",
        "* Add the `tfds.core.DatasetInfo` as second argument of `tfds.as_dataframe` to visualize images, audio, texts, videos,...\n",
        "* Use `ds.take(x)` to only display the first `x` examples. `pandas.DataFrame` will load the full dataset in-memory, and can be very expensive to display."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FKouwN_yVSGQ"
      },
      "outputs": [],
      "source": [
        "# ds, info = tfds.load('mnist', split='train', with_info=True)\n",
        "\n",
        "# tfds.as_dataframe(ds.take(4), info)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b-eDO_EXVGWC"
      },
      "source": [
        "### tfds.show_examples\n",
        "\n",
        "`tfds.show_examples` returns a `matplotlib.figure.Figure` (only image datasets supported now):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DpE2FD56cSQR"
      },
      "outputs": [],
      "source": [
        "# ds, info = tfds.load('mnist', split='train', with_info=True)\n",
        "\n",
        "# fig = tfds.show_examples(ds, info)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y0iVVStvk0oI"
      },
      "source": [
        "## Access the dataset metadata\n",
        "\n",
        "All builders include a `tfds.core.DatasetInfo` object containing the dataset metadata.\n",
        "\n",
        "It can be accessed through:\n",
        "\n",
        "*   The `tfds.load` API:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UgLgtcd1ljzt"
      },
      "outputs": [],
      "source": [
        "# ds, info = tfds.load('mnist', with_info=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XodyqNXrlxTM"
      },
      "source": [
        "*   The `tfds.core.DatasetBuilder` API:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nmq97QkilxeL"
      },
      "outputs": [],
      "source": [
        "# builder = tfds.builder('mnist')\n",
        "# info = builder.info"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zMGOk_ZsmPeu"
      },
      "source": [
        "The dataset info contains additional informations about the dataset (version, citation, homepage, description,...)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O-wLIKD-mZQT"
      },
      "outputs": [],
      "source": [
        "# print(info)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1zvAfRtwnAFk"
      },
      "source": [
        "### Features metadata (label names, image shape,...)\n",
        "\n",
        "Access the `tfds.features.FeatureDict`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RcyZXncqoFab"
      },
      "outputs": [],
      "source": [
        "# info.features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KAm9AV7loyw5"
      },
      "source": [
        "Number of classes, label names:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HhfzBH6qowpz"
      },
      "outputs": [],
      "source": [
        "# print(info.features[\"label\"].num_classes)\n",
        "# print(info.features[\"label\"].names)\n",
        "# print(info.features[\"label\"].int2str(7))  # Human readable version (8 -> 'cat')\n",
        "# print(info.features[\"label\"].str2int('7'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g5eWtk9ro_AK"
      },
      "source": [
        "Shapes, dtypes:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SergV_wQowLY"
      },
      "outputs": [],
      "source": [
        "# print(info.features.shape)\n",
        "# print(info.features.dtype)\n",
        "# print(info.features['image'].shape)\n",
        "# print(info.features['image'].dtype)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "thMOZ4IKm55N"
      },
      "source": [
        "### Split metadata (e.g. split names, number of examples,...)\n",
        "\n",
        "Access the `tfds.core.SplitDict`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FBbfwA8Sp4ax"
      },
      "outputs": [],
      "source": [
        "# print(info.splits)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EVw1UVYa2HgN"
      },
      "source": [
        "Available splits:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fRBieOOquDzX"
      },
      "outputs": [],
      "source": [
        "# print(list(info.splits.keys()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iHW0VfA0t3dO"
      },
      "source": [
        "Get info on individual split:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-h_OSpRsqKpP"
      },
      "outputs": [],
      "source": [
        "# print(info.splits['train'].num_examples)\n",
        "# print(info.splits['train'].filenames)\n",
        "# print(info.splits['train'].num_shards)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fWhSkHFNuLwW"
      },
      "source": [
        "It also works with the subsplit API:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HO5irBZ3uIzQ"
      },
      "outputs": [],
      "source": [
        "# print(info.splits['train[15%:75%]'].num_examples)\n",
        "# print(info.splits['train[15%:75%]'].file_instructions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YZp2XJwQQrI0"
      },
      "source": [
        "## Troubleshooting\n",
        "\n",
        "### Manual download (if download fails)\n",
        "\n",
        "If download fails for some reason (e.g. offline,...). You can always manually download the data yourself and place it in the `manual_dir` (defaults to `~/tensorflow_datasets/downloads/manual/`.\n",
        "\n",
        "To find out which urls to download, look into:\n",
        "\n",
        " * For new datasets (implemented as folder): [`tensorflow_datasets/`](https://github.com/tensorflow/datasets/tree/master/tensorflow_datasets/)`<type>/<dataset_name>/checksums.tsv`. For example: [`tensorflow_datasets/datasets/bool_q/checksums.tsv`](https://github.com/tensorflow/datasets/blob/master/tensorflow_datasets/datasets/bool_q/checksums.tsv).\n",
        "\n",
        "   You can find the dataset source location in [our catalog](https://www.tensorflow.org/datasets/catalog/overview).\n",
        " * For old datasets: [`tensorflow_datasets/url_checksums/<dataset_name>.txt`](https://github.com/tensorflow/datasets/tree/master/tensorflow_datasets/url_checksums)\n",
        "\n",
        "### Fixing `NonMatchingChecksumError`\n",
        "\n",
        "TFDS ensure determinism by validating the checksums of downloaded urls.\n",
        "If `NonMatchingChecksumError` is raised, might indicate:\n",
        "\n",
        "  * The website may be down (e.g. `503 status code`). Please check the url.\n",
        "  * For Google Drive URLs, try again later as Drive sometimes rejects downloads when too many people access the same URL. See [bug](https://github.com/tensorflow/datasets/issues/1482)\n",
        "  * The original datasets files may have been updated. In this case the TFDS dataset builder should be updated. Please open a new Github issue or PR:\n",
        "     * Register the new checksums with `tfds build --register_checksums`\n",
        "     * Eventually update the dataset generation code.\n",
        "     * Update the dataset `VERSION`\n",
        "     * Update the dataset `RELEASE_NOTES`: What caused the checksums to change ? Did some examples changed ?\n",
        "     * Make sure the dataset can still be built.\n",
        "     * Send us a PR\n",
        "\n",
        "Note: You can also inspect the downloaded file in `~/tensorflow_datasets/download/`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GmeeOokMODg2"
      },
      "source": [
        "## Citation\n",
        "\n",
        "If you're using `tensorflow-datasets` for a paper, please include the following citation, in addition to any citation specific to the used datasets (which can be found in the [dataset catalog](https://www.tensorflow.org/datasets/catalog/overview)).\n",
        "\n",
        "```\n",
        "@misc{TFDS,\n",
        "  title = { {TensorFlow Datasets}, A collection of ready-to-use datasets},\n",
        "  howpublished = {\\url{https://www.tensorflow.org/datasets}},\n",
        "}\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf ~/tensorflow_datasets/maquina_contrapropaganda\n"
      ],
      "metadata": {
        "id": "O8t0a7NJqIjn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# üß© Limpieza y redimensionado f√≠sico del dataset\n",
        "# ============================================================\n",
        "\n",
        "import os\n",
        "from PIL import Image\n",
        "\n",
        "base_dir = \"/content/recortes_letras\"\n",
        "target_size = (64, 64)\n",
        "\n",
        "for root, dirs, files in os.walk(base_dir):\n",
        "    for f in files:\n",
        "        if not f.lower().endswith((\".jpg\", \".jpeg\", \".png\")):\n",
        "            continue\n",
        "        path = os.path.join(root, f)\n",
        "        try:\n",
        "            im = Image.open(path).convert(\"RGB\")\n",
        "            im = im.resize(target_size, Image.LANCZOS)\n",
        "            im.save(path)\n",
        "        except Exception as e:\n",
        "            print(\"‚ö†Ô∏è Error con\", path, \"‚Üí\", e)\n",
        "\n",
        "print(\"‚úÖ Todas las im√°genes fueron redimensionadas f√≠sicamente a 64√ó64 px.\")\n"
      ],
      "metadata": {
        "id": "68MI6qm1pGOA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# üß© Verificador de dataset ‚Äî reconstruye solo si hay letras nuevas\n",
        "# ============================================================\n",
        "\n",
        "import os\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "# ruta base donde est√°n las letras (aj√∫stala si usas Drive)\n",
        "data_dir = '/content/recortes_letras'\n",
        "builder_dir = os.path.expanduser('~/tensorflow_datasets/maquina_contrapropaganda')\n",
        "\n",
        "# funci√≥n auxiliar para listar carpetas v√°lidas\n",
        "def contar_carpetas(path):\n",
        "    return sorted([d for d in os.listdir(path) if os.path.isdir(os.path.join(path, d))])\n",
        "\n",
        "# carpetas actuales detectadas\n",
        "carpetas_actuales = contar_carpetas(data_dir)\n",
        "num_actual = len(carpetas_actuales)\n",
        "\n",
        "# cu√°ntas clases ten√≠a el dataset anterior (si existe)\n",
        "prev_num = 0\n",
        "if os.path.exists(builder_dir):\n",
        "    try:\n",
        "        info = tfds.builder('maquina_contrapropaganda').info\n",
        "        prev_num = info.features[\"label\"].num_classes\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "print(f\"üì¶ Letras actuales detectadas: {carpetas_actuales}\")\n",
        "print(f\"üß† Dataset anterior: {prev_num} clases | Nuevo: {num_actual} clases\")\n",
        "\n",
        "# si hay nuevas letras, borrar dataset cacheado\n",
        "if num_actual > prev_num:\n",
        "    print(\"‚ö†Ô∏è Se detectaron nuevas letras. Regenerando dataset completo...\")\n",
        "    !rm -rf ~/tensorflow_datasets/maquina_contrapropaganda\n",
        "else:\n",
        "    print(\"‚úÖ No hay cambios en las clases, se mantiene el dataset anterior.\")\n"
      ],
      "metadata": {
        "id": "xvk2_ggOga1C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# üîç Verificaci√≥n f√≠sica de tama√±os reales en disco\n",
        "# ============================================================\n",
        "\n",
        "from PIL import Image\n",
        "import os\n",
        "\n",
        "base_dir = \"/content/recortes_letras\"\n",
        "malas = []\n",
        "\n",
        "for root, dirs, files in os.walk(base_dir):\n",
        "    for f in files:\n",
        "        if not f.lower().endswith((\".jpg\", \".jpeg\", \".png\")):\n",
        "            continue\n",
        "        path = os.path.join(root, f)\n",
        "        try:\n",
        "            with Image.open(path) as im:\n",
        "                if im.size != (64, 64):\n",
        "                    malas.append((path, im.size))\n",
        "        except Exception as e:\n",
        "            malas.append((path, \"‚ùå error\"))\n",
        "\n",
        "print(f\"Total de im√°genes fuera de tama√±o esperado: {len(malas)}\")\n",
        "for i, (p, s) in enumerate(malas[:10]):\n",
        "    print(f\"{i+1:02d}. {p} ‚Üí {s}\")\n"
      ],
      "metadata": {
        "id": "x88wjwnOp3d4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# üì¶ Custom Dataset ‚Äî M√°quina de Contrapropaganda\n",
        "# ============================================================\n",
        "\n",
        "import tensorflow_datasets as tfds\n",
        "import tensorflow as tf\n",
        "import os\n",
        "\n",
        "_DESCRIPTION = \"\"\"\n",
        "Dataset visual para el proyecto 'M√°quina de Contrapropaganda'.\n",
        "Contiene letras recortadas clasificadas por carpeta (A‚ÄìZ),\n",
        "extra√≠das de carteles propagand√≠sticos.\n",
        "\"\"\"\n",
        "\n",
        "_CITATION = \"\"\"\n",
        "@misc{rafita2025maquinacontrapropaganda,\n",
        "  title={M√°quina de Contrapropaganda Dataset},\n",
        "  author={Arce, Mateo},\n",
        "  year={2025},\n",
        "  howpublished={Rafita Studio / Universidad de Chile}\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "class MaquinaContrapropaganda(tfds.core.GeneratorBasedBuilder):\n",
        "    VERSION = tfds.core.Version('1.0.0')\n",
        "\n",
        "    def _info(self):\n",
        "        return tfds.core.DatasetInfo(\n",
        "            builder=self,\n",
        "            description=_DESCRIPTION,\n",
        "            features=tfds.features.FeaturesDict({\n",
        "                \"image\": tfds.features.Image(shape=(None, None, 3)),\n",
        "                \"label\": tfds.features.ClassLabel(names=[chr(i) for i in range(65, 91)])  # A‚ÄìZ\n",
        "            }),\n",
        "            supervised_keys=(\"image\", \"label\"),\n",
        "            citation=_CITATION,\n",
        "        )\n",
        "\n",
        "    def _split_generators(self, dl_manager):\n",
        "        data_dir = os.path.expanduser('/content/recortes_letras')\n",
        "        return {\"train\": self._generate_examples(data_dir)}\n",
        "\n",
        "    def _generate_examples(self, path):\n",
        "        for label_name in sorted(os.listdir(path)):\n",
        "            label_dir = os.path.join(path, label_name)\n",
        "            if not os.path.isdir(label_dir):\n",
        "                continue\n",
        "            for img_name in os.listdir(label_dir):\n",
        "                if img_name.lower().endswith((\".jpg\", \".png\", \".jpeg\")):\n",
        "                    yield img_name, {\n",
        "                        \"image\": os.path.join(label_dir, img_name),\n",
        "                        \"label\": label_name,\n",
        "                    }\n",
        "\n",
        "# === Construcci√≥n del dataset ===\n",
        "builder = MaquinaContrapropaganda()\n",
        "builder.download_and_prepare()\n",
        "\n",
        "ds = builder.as_dataset(split=\"train\", as_supervised=True)\n",
        "\n",
        "print(\"‚úÖ Dataset cargado correctamente.\")\n",
        "print(\"Clases detectadas:\", builder.info.features[\"label\"].names)\n",
        "\n"
      ],
      "metadata": {
        "id": "KRWDmAsjjcpo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# üëÅÔ∏è Visualizaci√≥n de ejemplos del dataset\n",
        "# ============================================================\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "for image, label in ds.take(9):\n",
        "    plt.figure(figsize=(2, 2))\n",
        "    plt.imshow(image)\n",
        "    plt.title(builder.info.features[\"label\"].int2str(label.numpy()))\n",
        "    plt.axis(\"off\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "326GWD0MjgM-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# üõ†Ô∏è Redimensionado f√≠sico forzado (solo las malas)\n",
        "# ============================================================\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "for path, size in malas:\n",
        "    try:\n",
        "        im = Image.open(path).convert(\"RGB\")\n",
        "        im = im.resize((64, 64), Image.LANCZOS)\n",
        "        im.save(path)\n",
        "    except Exception as e:\n",
        "        print(\"‚ùå No se pudo reparar:\", path)\n",
        "\n",
        "print(\"‚úÖ Todas las im√°genes malas fueron corregidas.\")\n"
      ],
      "metadata": {
        "id": "2L7UxxxkqCN3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# üß© Divisi√≥n autom√°tica del dataset en train / val / test\n",
        "# ============================================================\n",
        "\n",
        "import tensorflow as tf\n",
        "import math\n",
        "\n",
        "# tama√±o total del dataset\n",
        "total = sum(1 for _ in ds)\n",
        "train_size = math.floor(total * 0.8)\n",
        "val_size = math.floor(total * 0.1)\n",
        "test_size = total - train_size - val_size\n",
        "\n",
        "print(f\"üìä Total de ejemplos: {total}\")\n",
        "print(f\"üîπ Train: {train_size} | üî∏ Val: {val_size} | ‚ö™ Test: {test_size}\")\n",
        "\n",
        "# --- dividir usando el m√©todo take() y skip() ---\n",
        "train_ds = ds.take(train_size)\n",
        "val_ds = ds.skip(train_size).take(val_size)\n",
        "test_ds = ds.skip(train_size + val_size)\n",
        "\n",
        "# --- normalizar im√°genes ---\n",
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "\n",
        "def preprocess(img, label):\n",
        "    img = tf.image.convert_image_dtype(img, tf.float32)\n",
        "    return img, label\n",
        "\n",
        "train_ds = train_ds.map(preprocess).cache().shuffle(1000).batch(32).prefetch(AUTOTUNE)\n",
        "val_ds = val_ds.map(preprocess).cache().batch(32).prefetch(AUTOTUNE)\n",
        "test_ds = test_ds.map(preprocess).cache().batch(32).prefetch(AUTOTUNE)\n",
        "\n",
        "print(\"‚úÖ Datasets divididos y listos para entrenamiento.\")\n"
      ],
      "metadata": {
        "id": "AVAi_Fjtj1qd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# ‚úÖ Comprobaci√≥n de tama√±o de batch y forma de im√°genes\n",
        "# ============================================================\n",
        "\n",
        "for imgs, labels in train_ds.take(1):\n",
        "    print(\"‚úÖ batch shape:\", imgs.shape)\n",
        "    print(\"üîπ dtype:\", imgs.dtype)\n",
        "    print(\"üîπ rango de valores:\", tf.reduce_min(imgs).numpy(), \"‚Üí\", tf.reduce_max(imgs).numpy())\n",
        "\n",
        "    # muestra una de las im√°genes para confirmar visualmente\n",
        "    import matplotlib.pyplot as plt\n",
        "    plt.imshow(imgs[0])\n",
        "    plt.title(f\"Ejemplo de imagen ‚Äî tama√±o {imgs[0].shape}\")\n",
        "    plt.axis(\"off\")\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "b9B6Ri_QporQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# üß© Configuraci√≥n general\n",
        "# ============================================================\n",
        "\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "IMG_SIZE = 64\n",
        "EPOCHS = 40\n",
        "\n",
        "# ============================================================\n",
        "# üîß Dataset sin etiquetas y con repetici√≥n infinita\n",
        "# ============================================================\n",
        "\n",
        "def ensure_valid_image(img):\n",
        "    # normaliza y redimensiona cada imagen a 64x64\n",
        "    img = tf.image.convert_image_dtype(img, tf.float32)\n",
        "    img = tf.image.resize(img, [IMG_SIZE, IMG_SIZE])\n",
        "    return tf.ensure_shape(img, [IMG_SIZE, IMG_SIZE, 3])\n",
        "\n",
        "train_ds_no_labels = (\n",
        "    train_ds.unbatch()\n",
        "    .map(lambda x, y: ensure_valid_image(x), num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    .shuffle(512)\n",
        "    .batch(32)\n",
        "    .repeat()\n",
        "    .prefetch(tf.data.AUTOTUNE)\n",
        ")\n",
        "\n",
        "val_ds_no_labels = (\n",
        "    val_ds.unbatch()\n",
        "    .map(lambda x, y: ensure_valid_image(x), num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    .batch(32)\n",
        "    .repeat()\n",
        "    .prefetch(tf.data.AUTOTUNE)\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Datasets verificados:\")\n",
        "for imgs in train_ds_no_labels.take(1):\n",
        "    print(\"train batch:\", imgs.shape)\n",
        "for imgs in val_ds_no_labels.take(1):\n",
        "    print(\"val batch:\", imgs.shape)\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# üé® VisualCallback corregido (seguro y estable)\n",
        "# ============================================================\n",
        "\n",
        "class VisualCallback(tf.keras.callbacks.Callback):\n",
        "    def __init__(self, sample_batch, save_dir=\"/content/outputs\", interval=5):\n",
        "        super().__init__()\n",
        "        self.sample_batch = sample_batch\n",
        "        self.save_dir = save_dir\n",
        "        self.interval = interval\n",
        "        os.makedirs(save_dir, exist_ok=True)\n",
        "        self.generated_images = [] # List to store generated images for GIF\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        if (epoch + 1) % self.interval != 0:\n",
        "            return\n",
        "\n",
        "        sample_imgs = self.sample_batch[:8]\n",
        "        z_mean, z_log_var, z = self.model.encoder(sample_imgs)\n",
        "        reconstructed = self.model.decoder(z)\n",
        "\n",
        "        n = 8\n",
        "        fig, axes = plt.subplots(2, n, figsize=(n * 1.5, 3))\n",
        "        for i in range(n):\n",
        "            axes[0, i].imshow(sample_imgs[i])\n",
        "            axes[0, i].axis(\"off\")\n",
        "            axes[1, i].imshow(reconstructed[i])\n",
        "            axes[1, i].axis(\"off\")\n",
        "        plt.tight_layout()\n",
        "\n",
        "        # Save the figure as an image for later GIF creation\n",
        "        path = os.path.join(self.save_dir, f\"epoch_{epoch+1:03d}.png\")\n",
        "        plt.savefig(path)\n",
        "        plt.close(fig)\n",
        "        print(f\"üåÄ Letras alucinadas guardadas en: {path}\")\n",
        "\n",
        "        # Display the generated images live\n",
        "        plt.figure(figsize=(n * 1.5, 3))\n",
        "        for i in range(n):\n",
        "             plt.subplot(2, n, i + 1)\n",
        "             plt.imshow(sample_imgs[i])\n",
        "             plt.axis(\"off\")\n",
        "             plt.subplot(2, n, i + n + 1)\n",
        "             plt.imshow(reconstructed[i])\n",
        "             plt.axis(\"off\")\n",
        "        plt.suptitle(f\"Epoch {epoch+1}\", fontsize=16)\n",
        "        plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "        # Store the generated image batch for GIF creation\n",
        "        self.generated_images.append(reconstructed.numpy())\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# ‚öôÔ∏è Definici√≥n de p√©rdida del VAE\n",
        "# ============================================================\n",
        "\n",
        "# This loss function is no longer directly used by vae.fit because\n",
        "# we define a custom train_step in the VAE model.\n",
        "def vae_total_loss(y_true, y_pred):\n",
        "    reconstruction_loss = tf.reduce_mean(\n",
        "        tf.keras.losses.binary_crossentropy(y_true, y_pred)\n",
        "    ) * IMG_SIZE * IMG_SIZE * 3\n",
        "    # KL divergence loss is calculated in the train_step\n",
        "    return reconstruction_loss # This will be combined with KL loss in train_step\n",
        "\n",
        "\n",
        "# # ============================================================\n",
        "# # üß† Entrenamiento del VAE (versi√≥n estable) - DEPRECATED\n",
        "# # ============================================================\n",
        "\n",
        "# # obtenemos un batch de muestra para el callback\n",
        "# sample_batch = next(iter(train_ds_no_labels))\n",
        "\n",
        "# vae = VAE(encoder, decoder)\n",
        "# vae.compile(optimizer=tf.keras.optimizers.Adam(), loss=vae_total_loss)\n",
        "\n",
        "# vae.fit(\n",
        "#     train_ds_no_labels,\n",
        "#     validation_data=val_ds_no_labels,\n",
        "#     epochs=EPOCHS,\n",
        "#     steps_per_epoch=50,\n",
        "#     validation_steps=10,\n",
        "#     callbacks=[VisualCallback(sample_batch)],\n",
        "#     verbose=1\n",
        "# )\n",
        "\n",
        "\n",
        "# # ============================================================\n",
        "# # üíæ Guardado de modelos entrenados - DEPRECATED\n",
        "# # ============================================================\n",
        "\n",
        "# decoder.save(\"/content/drive/MyDrive/maquina-de-contrapropaganda/models/decoder_solo.keras\")\n",
        "# encoder.save(\"/content/drive/MyDrive/maquina-de-contrapropaganda/models/encoder_solo.keras\")\n",
        "# vae.save(\"/content/drive/MyDrive/maquina-de-contrapropaganda/models/vae_completo.keras\")\n",
        "\n",
        "# print(\"‚úÖ Modelos guardados correctamente en Drive.\")"
      ],
      "metadata": {
        "id": "L_CNHl2ElcHu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3663a9aa"
      },
      "source": [
        "# ============================================================\n",
        "# üß† Definici√≥n del Encoder (versi√≥n estable)\n",
        "# ============================================================\n",
        "\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import tensorflow as tf\n",
        "\n",
        "def sampling(args):\n",
        "    z_mean, z_log_var = args\n",
        "    batch = tf.shape(z_mean)[0]\n",
        "    dim = tf.shape(z_mean)[1]\n",
        "    epsilon = tf.random.normal(shape=(batch, dim))\n",
        "    return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
        "\n",
        "# Encoder network\n",
        "# Input shape is IMG_SIZE x IMG_SIZE x 3 (64x64x3)\n",
        "encoder_inputs = keras.Input(shape=(IMG_SIZE, IMG_SIZE, 3))\n",
        "x = layers.Conv2D(32, 3, activation=\"relu\", strides=2, padding=\"same\")(encoder_inputs)\n",
        "x = layers.Conv2D(64, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\n",
        "x = layers.Flatten()(x)\n",
        "x = layers.Dense(256, activation=\"relu\")(x)\n",
        "z_mean = layers.Dense(LATENT_DIM, name=\"z_mean\")(x)\n",
        "z_log_var = layers.Dense(LATENT_DIM, name=\"z_log_var\")(x)\n",
        "z = layers.Lambda(sampling, name=\"z\")([z_mean, z_log_var])\n",
        "encoder = keras.Model(encoder_inputs, [z_mean, z_log_var, z], name=\"encoder\")\n",
        "encoder.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "af82f471"
      },
      "source": [
        "# ============================================================\n",
        "# üß† Definici√≥n del Decoder (versi√≥n estable)\n",
        "# ============================================================\n",
        "\n",
        "# Decoder network\n",
        "latent_inputs = keras.Input(shape=(LATENT_DIM,))\n",
        "x = layers.Dense(8 * 8 * 64, activation=\"relu\")(latent_inputs) # Adjusted dense layer output\n",
        "x = layers.Reshape((8, 8, 64))(x) # Adjusted reshape\n",
        "x = layers.Conv2DTranspose(64, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\n",
        "x = layers.Conv2DTranspose(32, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\n",
        "# Added another Conv2DTranspose layer to reach 64x64\n",
        "x = layers.Conv2DTranspose(3, 3, activation=\"sigmoid\", strides=2, padding=\"same\")(x)\n",
        "decoder_outputs = x # Final output\n",
        "decoder = keras.Model(latent_inputs, decoder_outputs, name=\"decoder\")\n",
        "decoder.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bb386900"
      },
      "source": [
        "# ============================================================\n",
        "# üß† Definici√≥n del VAE Model con train_step (versi√≥n estable)\n",
        "# ============================================================\n",
        "\n",
        "from tensorflow import keras\n",
        "import tensorflow as tf\n",
        "\n",
        "class VAE(keras.Model):\n",
        "    def __init__(self, encoder, decoder, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "\n",
        "    def call(self, inputs):\n",
        "        z_mean, z_log_var, z = self.encoder(inputs)\n",
        "        reconstructed = self.decoder(z)\n",
        "        return reconstructed\n",
        "\n",
        "    # Define the training step\n",
        "    def train_step(self, data):\n",
        "        # The dataset is yielding only images\n",
        "        images = data\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "            z_mean, z_log_var, z = self.encoder(images)\n",
        "            reconstructed_images = self.decoder(z)\n",
        "\n",
        "            # Calculate reconstruction loss\n",
        "            reconstruction_loss = tf.reduce_mean(\n",
        "                tf.keras.losses.binary_crossentropy(images, reconstructed_images)\n",
        "            ) * IMG_SIZE * IMG_SIZE * 3  # Scale by image dimensions\n",
        "\n",
        "            # Calculate KL divergence loss\n",
        "            kl_loss = -0.5 * tf.reduce_mean(z_log_var - tf.square(z_mean) - tf.exp(z_log_var) + 1)\n",
        "\n",
        "            # Total VAE loss\n",
        "            total_loss = reconstruction_loss + kl_loss\n",
        "\n",
        "        grads = tape.gradient(total_loss, self.trainable_weights)\n",
        "        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
        "\n",
        "        return {\n",
        "            \"loss\": total_loss,\n",
        "            \"reconstruction_loss\": reconstruction_loss,\n",
        "            \"kl_loss\": kl_loss,\n",
        "        }\n",
        "\n",
        "    # Define the test step for validation/evaluation\n",
        "    def test_step(self, data):\n",
        "        images = data\n",
        "\n",
        "        z_mean, z_log_var, z = self.encoder(images)\n",
        "        reconstructed_images = self.decoder(z)\n",
        "\n",
        "        reconstruction_loss = tf.reduce_mean(\n",
        "            tf.keras.losses.binary_crossentropy(images, reconstructed_images)\n",
        "        ) * IMG_SIZE * IMG_SIZE * 3\n",
        "\n",
        "        kl_loss = -0.5 * tf.reduce_mean(z_log_var - tf.square(z_mean) - tf.exp(z_log_var) + 1)\n",
        "\n",
        "        total_loss = reconstruction_loss + kl_loss\n",
        "\n",
        "        return {\n",
        "            \"loss\": total_loss,\n",
        "            \"reconstruction_loss\": reconstruction_loss,\n",
        "            \"kl_loss\": kl_loss,\n",
        "        }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5841da50"
      },
      "source": [
        "# ============================================================\n",
        "# üß† Entrenamiento del VAE (versi√≥n estable con train_step)\n",
        "# ============================================================\n",
        "\n",
        "# obtenemos un batch de muestra para el callback\n",
        "sample_batch = next(iter(train_ds_no_labels))\n",
        "\n",
        "# Instantiate the VAE model\n",
        "vae = VAE(encoder, decoder)\n",
        "\n",
        "# Compile the VAE (loss and metrics are handled in train_step)\n",
        "vae.compile(optimizer=tf.keras.optimizers.Adam())\n",
        "\n",
        "# Instantiate the VisualCallback\n",
        "visual_callback = VisualCallback(sample_batch)\n",
        "\n",
        "print(\"Starting VAE training...\")\n",
        "history = vae.fit(\n",
        "    train_ds_no_labels,\n",
        "    validation_data=val_ds_no_labels,\n",
        "    epochs=EPOCHS,\n",
        "    steps_per_epoch=50,\n",
        "    validation_steps=10,\n",
        "    callbacks=[visual_callback], # Use the instantiated callback\n",
        "    verbose=1\n",
        ")\n",
        "print(\"VAE training finished.\")\n",
        "\n",
        "# ============================================================\n",
        "# üñºÔ∏è Generar GIF de la evoluci√≥n de las letras\n",
        "# ============================================================\n",
        "\n",
        "import imageio\n",
        "\n",
        "# Assuming the generated images are stored in visual_callback.generated_images\n",
        "# Convert the list of numpy arrays to a format imageio can handle (list of images)\n",
        "# Each element in generated_images is a batch (batch_size, 64, 64, 3)\n",
        "# We need to select the images we want to include in the GIF, e.g., the first 8\n",
        "gif_images = []\n",
        "for batch in visual_callback.generated_images:\n",
        "    # Take the first 8 images from each batch and convert to uint8\n",
        "    gif_images.extend([np.uint8(img * 255) for img in batch[:8]])\n",
        "\n",
        "# Save the GIF\n",
        "gif_path = \"/content/vae_evolution.gif\"\n",
        "imageio.mimsave(gif_path, gif_images, fps=1) # Adjust fps as needed\n",
        "\n",
        "print(f\"‚úÖ GIF de la evoluci√≥n guardado en: {gif_path}\")\n",
        "\n",
        "# Display the GIF in the notebook\n",
        "from IPython.display import Image as IPyImage\n",
        "IPyImage(open(gif_path,'rb').read())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6af32587"
      },
      "source": [
        "# ============================================================\n",
        "# üñºÔ∏è Generar nuevas letras desde el espacio latente\n",
        "# ============================================================\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import imageio\n",
        "import os\n",
        "\n",
        "# N√∫mero de nuevas letras a generar\n",
        "num_new_letters = 16 # Let's generate 16 new letters\n",
        "\n",
        "# Directorio para guardar las im√°genes generadas para el GIF\n",
        "generate_dir = \"/content/generated_letters\"\n",
        "os.makedirs(generate_dir, exist_ok=True)\n",
        "\n",
        "# Lista para almacenar las im√°genes generadas para el GIF\n",
        "gif_frames = []\n",
        "\n",
        "print(f\"Generating {num_new_letters} new letters from the latent space...\")\n",
        "\n",
        "# Generate images over a few steps to simulate evolution for the GIF\n",
        "num_generation_steps = 10 # Number of frames for the GIF per letter\n",
        "\n",
        "# Sample latent vectors once\n",
        "random_latent_vectors = tf.random.normal(shape=(num_new_letters, LATENT_DIM))\n",
        "\n",
        "for step in range(num_generation_steps):\n",
        "    # You could potentially add noise or interpolate here for a more dynamic GIF\n",
        "    # For simplicity, we will just generate the final images repeatedly for the frames\n",
        "    generated_images = decoder(random_latent_vectors).numpy()\n",
        "\n",
        "    # Create a figure to display the generated images\n",
        "    n = int(np.sqrt(num_new_letters))\n",
        "    fig, axes = plt.subplots(n, n, figsize=(n * 2, n * 2))\n",
        "    axes = axes.flatten()\n",
        "\n",
        "    for i in range(num_new_letters):\n",
        "        axes[i].imshow(generated_images[i])\n",
        "        axes[i].axis(\"off\")\n",
        "\n",
        "    plt.suptitle(f\"Generation Step {step+1}/{num_generation_steps}\", fontsize=16)\n",
        "    plt.tight_layout(rect=[0, 0.03, 1, 0.95]) # Adjust layout to prevent title overlap\n",
        "\n",
        "    # Save the figure as an image frame for the GIF\n",
        "    frame_path = os.path.join(generate_dir, f\"generation_step_{step+1:03d}.png\")\n",
        "    plt.savefig(frame_path)\n",
        "    plt.close(fig)\n",
        "\n",
        "    # Append the generated images (as uint8) to the list for the GIF\n",
        "    # We'll just take the first few for the GIF to keep it manageable\n",
        "    gif_frames.append(np.uint8(generated_images[:min(num_new_letters, 16)] * 255))\n",
        "\n",
        "\n",
        "print(\"Finished generating image frames.\")\n",
        "\n",
        "# Create the GIF from the saved frames\n",
        "gif_path = \"/content/new_letters_evolution.gif\"\n",
        "\n",
        "# Need to flatten the list of batches into a single list of images for imageio.mimsave\n",
        "flat_gif_frames = [img for batch in gif_frames for img in batch]\n",
        "\n",
        "imageio.mimsave(gif_path, flat_gif_frames, fps=5) # Adjust fps as needed\n",
        "\n",
        "print(f\"‚úÖ GIF of new letter generation evolution saved to: {gif_path}\")\n",
        "\n",
        "# Display the GIF in the notebook\n",
        "from IPython.display import Image as IPyImage\n",
        "IPyImage(open(gif_path,'rb').read())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f22bde8a"
      },
      "source": [
        "# ============================================================\n",
        "# üß† Entrenamiento del VAE (versi√≥n estable con train_step)\n",
        "# ============================================================\n",
        "\n",
        "# obtenemos un batch de muestra para el callback\n",
        "sample_batch = next(iter(train_ds_no_labels))\n",
        "\n",
        "# Instantiate the VAE model\n",
        "vae = VAE(encoder, decoder)\n",
        "\n",
        "# Compile the VAE (loss and metrics are handled in train_step)\n",
        "vae.compile(optimizer=tf.keras.optimizers.Adam())\n",
        "\n",
        "# Instantiate the VisualCallback\n",
        "visual_callback = VisualCallback(sample_batch)\n",
        "\n",
        "print(\"Starting VAE training...\")\n",
        "history = vae.fit(\n",
        "    train_ds_no_labels,\n",
        "    validation_data=val_ds_no_labels,\n",
        "    epochs=EPOCHS,\n",
        "    steps_per_epoch=50,\n",
        "    validation_steps=10,\n",
        "    callbacks=[visual_callback], # Use the instantiated callback\n",
        "    verbose=1\n",
        ")\n",
        "print(\"VAE training finished.\")\n",
        "\n",
        "# ============================================================\n",
        "# üñºÔ∏è Generar GIF de la evoluci√≥n de las letras\n",
        "# ============================================================\n",
        "\n",
        "import imageio\n",
        "\n",
        "# Assuming the generated images are stored in visual_callback.generated_images\n",
        "# Convert the list of numpy arrays to a format imageio can handle (list of images)\n",
        "# Each element in generated_images is a batch (batch_size, 64, 64, 3)\n",
        "# We need to select the images we want to include in the GIF, e.g., the first 8\n",
        "gif_images = []\n",
        "for batch in visual_callback.generated_images:\n",
        "    # Take the first 8 images from each batch and convert to uint8\n",
        "    gif_images.extend([np.uint8(img * 255) for img in batch[:8]])\n",
        "\n",
        "# Save the GIF\n",
        "gif_path = \"/content/vae_evolution.gif\"\n",
        "imageio.mimsave(gif_path, gif_images, fps=1) # Adjust fps as needed\n",
        "\n",
        "print(f\"‚úÖ GIF de la evoluci√≥n guardado en: {gif_path}\")\n",
        "\n",
        "# Display the GIF in the notebook\n",
        "from IPython.display import Image as IPyImage\n",
        "IPyImage(open(gif_path,'rb').read())"
      ],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}