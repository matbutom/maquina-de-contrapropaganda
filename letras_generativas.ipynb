{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/matbutom/maquina-de-contrapropaganda/blob/main/letras_generativas.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "# Crea primero el directorio base si no existe\n",
        "mkdir -p /content/recortes_letras\n",
        "\n",
        "# Crea los subdirectorios de A a Z\n",
        "for i in {A..Z}; do\n",
        "  mkdir -p /content/recortes_letras/$i\n",
        "done"
      ],
      "metadata": {
        "id": "pOFvdL0ifHFe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "\n",
        "# 1. Definir el URL base de tu repositorio de GitHub\n",
        "REPO_URL=\"https://github.com/matbutom/maquina-de-contrapropaganda.git\"\n",
        "REPO_NAME=\"maquina-de-contrapropaganda\"\n",
        "TARGET_DIR=\"/content/recortes_letras\"\n",
        "\n",
        "echo \"Clonando el repositorio completo ($REPO_NAME) en el entorno de Colab...\"\n",
        "# Clonar el repositorio\n",
        "git clone $REPO_URL\n",
        "\n",
        "# 2. Mover las carpetas con im√°genes al directorio de trabajo (recortes_letras)\n",
        "SOURCE_CONTENT=\"$REPO_NAME/recortes_letras/*\"\n",
        "\n",
        "echo \"Moviendo el contenido de las carpetas de letras (A, B, C...) a $TARGET_DIR...\"\n",
        "# 'cp -r' copia recursivamente el contenido de las subcarpetas A-Z\n",
        "cp -r $SOURCE_CONTENT $TARGET_DIR/\n",
        "\n",
        "# 3. Limpiar el repositorio clonado (ya no se necesita)\n",
        "echo \"Limpiando el repositorio clonado...\"\n",
        "rm -rf $REPO_NAME\n",
        "\n",
        "# 4. Verificaci√≥n: Mostrar el contenido de la carpeta 'A' para confirmar que las im√°genes se cargaron\n",
        "echo \"‚úÖ ¬°Carga completa! Verificando la carpeta 'A':\"\n",
        "ls -l $TARGET_DIR/A | head -n 5"
      ],
      "metadata": {
        "id": "SoBlofRWgy4R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf ~/tensorflow_datasets/maquina_contrapropaganda\n"
      ],
      "metadata": {
        "id": "O8t0a7NJqIjn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# üß© Limpieza y redimensionado f√≠sico del dataset\n",
        "# ============================================================\n",
        "\n",
        "import os\n",
        "from PIL import Image\n",
        "\n",
        "base_dir = \"/content/recortes_letras\"\n",
        "target_size = (64, 64)\n",
        "\n",
        "for root, dirs, files in os.walk(base_dir):\n",
        "    for f in files:\n",
        "        if not f.lower().endswith((\".jpg\", \".jpeg\", \".png\")):\n",
        "            continue\n",
        "        path = os.path.join(root, f)\n",
        "        try:\n",
        "            im = Image.open(path).convert(\"RGB\")\n",
        "            im = im.resize(target_size, Image.LANCZOS)\n",
        "            im.save(path)\n",
        "        except Exception as e:\n",
        "            print(\"‚ö†Ô∏è Error con\", path, \"‚Üí\", e)\n",
        "\n",
        "print(\"‚úÖ Todas las im√°genes fueron redimensionadas f√≠sicamente a 64√ó64 px.\")\n"
      ],
      "metadata": {
        "id": "68MI6qm1pGOA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# üß© Verificador de dataset ‚Äî reconstruye solo si hay letras nuevas\n",
        "# ============================================================\n",
        "\n",
        "import os\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "# ruta base donde est√°n las letras (aj√∫stala si usas Drive)\n",
        "data_dir = '/content/recortes_letras'\n",
        "builder_dir = os.path.expanduser('~/tensorflow_datasets/maquina_contrapropaganda')\n",
        "\n",
        "# funci√≥n auxiliar para listar carpetas v√°lidas\n",
        "def contar_carpetas(path):\n",
        "    return sorted([d for d in os.listdir(path) if os.path.isdir(os.path.join(path, d))])\n",
        "\n",
        "# carpetas actuales detectadas\n",
        "carpetas_actuales = contar_carpetas(data_dir)\n",
        "num_actual = len(carpetas_actuales)\n",
        "\n",
        "# cu√°ntas clases ten√≠a el dataset anterior (si existe)\n",
        "prev_num = 0\n",
        "if os.path.exists(builder_dir):\n",
        "    try:\n",
        "        info = tfds.builder('maquina_contrapropaganda').info\n",
        "        prev_num = info.features[\"label\"].num_classes\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "print(f\"üì¶ Letras actuales detectadas: {carpetas_actuales}\")\n",
        "print(f\"üß† Dataset anterior: {prev_num} clases | Nuevo: {num_actual} clases\")\n",
        "\n",
        "# si hay nuevas letras, borrar dataset cacheado\n",
        "if num_actual > prev_num:\n",
        "    print(\"‚ö†Ô∏è Se detectaron nuevas letras. Regenerando dataset completo...\")\n",
        "    !rm -rf ~/tensorflow_datasets/maquina_contrapropaganda\n",
        "else:\n",
        "    print(\"‚úÖ No hay cambios en las clases, se mantiene el dataset anterior.\")\n"
      ],
      "metadata": {
        "id": "xvk2_ggOga1C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# üîç Verificaci√≥n f√≠sica de tama√±os reales en disco\n",
        "# ============================================================\n",
        "\n",
        "from PIL import Image\n",
        "import os\n",
        "\n",
        "base_dir = \"/content/recortes_letras\"\n",
        "malas = []\n",
        "\n",
        "for root, dirs, files in os.walk(base_dir):\n",
        "    for f in files:\n",
        "        if not f.lower().endswith((\".jpg\", \".jpeg\", \".png\")):\n",
        "            continue\n",
        "        path = os.path.join(root, f)\n",
        "        try:\n",
        "            with Image.open(path) as im:\n",
        "                if im.size != (64, 64):\n",
        "                    malas.append((path, im.size))\n",
        "        except Exception as e:\n",
        "            malas.append((path, \"‚ùå error\"))\n",
        "\n",
        "print(f\"Total de im√°genes fuera de tama√±o esperado: {len(malas)}\")\n",
        "for i, (p, s) in enumerate(malas[:10]):\n",
        "    print(f\"{i+1:02d}. {p} ‚Üí {s}\")\n"
      ],
      "metadata": {
        "id": "x88wjwnOp3d4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# üì¶ Custom Dataset ‚Äî M√°quina de Contrapropaganda\n",
        "# ============================================================\n",
        "\n",
        "import tensorflow_datasets as tfds\n",
        "import tensorflow as tf\n",
        "import os\n",
        "\n",
        "_DESCRIPTION = \"\"\"\n",
        "Dataset visual para el proyecto 'M√°quina de Contrapropaganda'.\n",
        "Contiene letras recortadas clasificadas por carpeta (A‚ÄìZ),\n",
        "extra√≠das de carteles propagand√≠sticos.\n",
        "\"\"\"\n",
        "\n",
        "_CITATION = \"\"\"\n",
        "@misc{rafita2025maquinacontrapropaganda,\n",
        "  title={M√°quina de Contrapropaganda Dataset},\n",
        "  author={Arce, Mateo},\n",
        "  year={2025},\n",
        "  howpublished={Rafita Studio / Universidad de Chile}\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "class MaquinaContrapropaganda(tfds.core.GeneratorBasedBuilder):\n",
        "    VERSION = tfds.core.Version('1.0.0')\n",
        "\n",
        "    def _info(self):\n",
        "        return tfds.core.DatasetInfo(\n",
        "            builder=self,\n",
        "            description=_DESCRIPTION,\n",
        "            features=tfds.features.FeaturesDict({\n",
        "                \"image\": tfds.features.Image(shape=(None, None, 3)),\n",
        "                \"label\": tfds.features.ClassLabel(names=[chr(i) for i in range(65, 91)])  # A‚ÄìZ\n",
        "            }),\n",
        "            supervised_keys=(\"image\", \"label\"),\n",
        "            citation=_CITATION,\n",
        "        )\n",
        "\n",
        "    def _split_generators(self, dl_manager):\n",
        "        data_dir = os.path.expanduser('/content/recortes_letras')\n",
        "        return {\"train\": self._generate_examples(data_dir)}\n",
        "\n",
        "    def _generate_examples(self, path):\n",
        "        for label_name in sorted(os.listdir(path)):\n",
        "            label_dir = os.path.join(path, label_name)\n",
        "            if not os.path.isdir(label_dir):\n",
        "                continue\n",
        "            for img_name in os.listdir(label_dir):\n",
        "                if img_name.lower().endswith((\".jpg\", \".png\", \".jpeg\")):\n",
        "                    yield img_name, {\n",
        "                        \"image\": os.path.join(label_dir, img_name),\n",
        "                        \"label\": label_name,\n",
        "                    }\n",
        "\n",
        "# === Construcci√≥n del dataset ===\n",
        "builder = MaquinaContrapropaganda()\n",
        "builder.download_and_prepare()\n",
        "\n",
        "ds = builder.as_dataset(split=\"train\", as_supervised=True)\n",
        "\n",
        "print(\"‚úÖ Dataset cargado correctamente.\")\n",
        "print(\"Clases detectadas:\", builder.info.features[\"label\"].names)\n",
        "\n"
      ],
      "metadata": {
        "id": "KRWDmAsjjcpo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# üëÅÔ∏è Visualizaci√≥n de ejemplos del dataset\n",
        "# ============================================================\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "for image, label in ds.take(9):\n",
        "    plt.figure(figsize=(2, 2))\n",
        "    plt.imshow(image)\n",
        "    plt.title(builder.info.features[\"label\"].int2str(label.numpy()))\n",
        "    plt.axis(\"off\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "326GWD0MjgM-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# üõ†Ô∏è Redimensionado f√≠sico forzado (solo las malas)\n",
        "# ============================================================\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "for path, size in malas:\n",
        "    try:\n",
        "        im = Image.open(path).convert(\"RGB\")\n",
        "        im = im.resize((64, 64), Image.LANCZOS)\n",
        "        im.save(path)\n",
        "    except Exception as e:\n",
        "        print(\"‚ùå No se pudo reparar:\", path)\n",
        "\n",
        "print(\"‚úÖ Todas las im√°genes malas fueron corregidas.\")\n"
      ],
      "metadata": {
        "id": "2L7UxxxkqCN3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# üß© Divisi√≥n autom√°tica del dataset en train / val / test\n",
        "# ============================================================\n",
        "\n",
        "import tensorflow as tf\n",
        "import math\n",
        "\n",
        "# tama√±o total del dataset\n",
        "total = sum(1 for _ in ds)\n",
        "train_size = math.floor(total * 0.8)\n",
        "val_size = math.floor(total * 0.1)\n",
        "test_size = total - train_size - val_size\n",
        "\n",
        "print(f\"üìä Total de ejemplos: {total}\")\n",
        "print(f\"üîπ Train: {train_size} | üî∏ Val: {val_size} | ‚ö™ Test: {test_size}\")\n",
        "\n",
        "# --- dividir usando el m√©todo take() y skip() ---\n",
        "train_ds = ds.take(train_size)\n",
        "val_ds = ds.skip(train_size).take(val_size)\n",
        "test_ds = ds.skip(train_size + val_size)\n",
        "\n",
        "# --- normalizar im√°genes ---\n",
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "\n",
        "def preprocess(img, label):\n",
        "    img = tf.image.convert_image_dtype(img, tf.float32)\n",
        "    return img, label\n",
        "\n",
        "train_ds = train_ds.map(preprocess).cache().shuffle(1000).batch(32).prefetch(AUTOTUNE)\n",
        "val_ds = val_ds.map(preprocess).cache().batch(32).prefetch(AUTOTUNE)\n",
        "test_ds = test_ds.map(preprocess).cache().batch(32).prefetch(AUTOTUNE)\n",
        "\n",
        "print(\"‚úÖ Datasets divididos y listos para entrenamiento.\")\n"
      ],
      "metadata": {
        "id": "AVAi_Fjtj1qd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# ‚úÖ Comprobaci√≥n de tama√±o de batch y forma de im√°genes\n",
        "# ============================================================\n",
        "\n",
        "for imgs, labels in train_ds.take(1):\n",
        "    print(\"‚úÖ batch shape:\", imgs.shape)\n",
        "    print(\"üîπ dtype:\", imgs.dtype)\n",
        "    print(\"üîπ rango de valores:\", tf.reduce_min(imgs).numpy(), \"‚Üí\", tf.reduce_max(imgs).numpy())\n",
        "\n",
        "    # muestra una de las im√°genes para confirmar visualmente\n",
        "    import matplotlib.pyplot as plt\n",
        "    plt.imshow(imgs[0])\n",
        "    plt.title(f\"Ejemplo de imagen ‚Äî tama√±o {imgs[0].shape}\")\n",
        "    plt.axis(\"off\")\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "b9B6Ri_QporQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# üß© Configuraci√≥n general\n",
        "# ============================================================\n",
        "\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "IMG_SIZE = 64\n",
        "EPOCHS = 40\n",
        "\n",
        "# ============================================================\n",
        "# üîß Dataset sin etiquetas y con repetici√≥n infinita\n",
        "# ============================================================\n",
        "\n",
        "def ensure_valid_image(img):\n",
        "    # normaliza y redimensiona cada imagen a 64x64\n",
        "    img = tf.image.convert_image_dtype(img, tf.float32)\n",
        "    img = tf.image.resize(img, [IMG_SIZE, IMG_SIZE])\n",
        "    return tf.ensure_shape(img, [IMG_SIZE, IMG_SIZE, 3])\n",
        "\n",
        "train_ds_no_labels = (\n",
        "    train_ds.unbatch()\n",
        "    .map(lambda x, y: ensure_valid_image(x), num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    .shuffle(512)\n",
        "    .batch(32)\n",
        "    .repeat()\n",
        "    .prefetch(tf.data.AUTOTUNE)\n",
        ")\n",
        "\n",
        "val_ds_no_labels = (\n",
        "    val_ds.unbatch()\n",
        "    .map(lambda x, y: ensure_valid_image(x), num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    .batch(32)\n",
        "    .repeat()\n",
        "    .prefetch(tf.data.AUTOTUNE)\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Datasets verificados:\")\n",
        "for imgs in train_ds_no_labels.take(1):\n",
        "    print(\"train batch:\", imgs.shape)\n",
        "for imgs in val_ds_no_labels.take(1):\n",
        "    print(\"val batch:\", imgs.shape)\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# üé® VisualCallback corregido (seguro y estable)\n",
        "# ============================================================\n",
        "\n",
        "class VisualCallback(tf.keras.callbacks.Callback):\n",
        "    def __init__(self, sample_batch, save_dir=\"/content/outputs\", interval=5):\n",
        "        super().__init__()\n",
        "        self.sample_batch = sample_batch\n",
        "        self.save_dir = save_dir\n",
        "        self.interval = interval\n",
        "        os.makedirs(save_dir, exist_ok=True)\n",
        "        self.generated_images = [] # List to store generated images for GIF\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        if (epoch + 1) % self.interval != 0:\n",
        "            return\n",
        "\n",
        "        sample_imgs = self.sample_batch[:8]\n",
        "        z_mean, z_log_var, z = self.model.encoder(sample_imgs)\n",
        "        reconstructed = self.model.decoder(z)\n",
        "\n",
        "        n = 8\n",
        "        fig, axes = plt.subplots(2, n, figsize=(n * 1.5, 3))\n",
        "        for i in range(n):\n",
        "            axes[0, i].imshow(sample_imgs[i])\n",
        "            axes[0, i].axis(\"off\")\n",
        "            axes[1, i].imshow(reconstructed[i])\n",
        "            axes[1, i].axis(\"off\")\n",
        "        plt.tight_layout()\n",
        "\n",
        "        # Save the figure as an image for later GIF creation\n",
        "        path = os.path.join(self.save_dir, f\"epoch_{epoch+1:03d}.png\")\n",
        "        plt.savefig(path)\n",
        "        plt.close(fig)\n",
        "        print(f\"üåÄ Letras alucinadas guardadas en: {path}\")\n",
        "\n",
        "        # Display the generated images live\n",
        "        plt.figure(figsize=(n * 1.5, 3))\n",
        "        for i in range(n):\n",
        "             plt.subplot(2, n, i + 1)\n",
        "             plt.imshow(sample_imgs[i])\n",
        "             plt.axis(\"off\")\n",
        "             plt.subplot(2, n, i + n + 1)\n",
        "             plt.imshow(reconstructed[i])\n",
        "             plt.axis(\"off\")\n",
        "        plt.suptitle(f\"Epoch {epoch+1}\", fontsize=16)\n",
        "        plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "        # Store the generated image batch for GIF creation\n",
        "        self.generated_images.append(reconstructed.numpy())\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# ‚öôÔ∏è Definici√≥n de p√©rdida del VAE\n",
        "# ============================================================\n",
        "\n",
        "# This loss function is no longer directly used by vae.fit because\n",
        "# we define a custom train_step in the VAE model.\n",
        "def vae_total_loss(y_true, y_pred):\n",
        "    reconstruction_loss = tf.reduce_mean(\n",
        "        tf.keras.losses.binary_crossentropy(y_true, y_pred)\n",
        "    ) * IMG_SIZE * IMG_SIZE * 3\n",
        "    # KL divergence loss is calculated in the train_step\n",
        "    return reconstruction_loss # This will be combined with KL loss in train_step\n",
        "\n",
        "\n",
        "# # ============================================================\n",
        "# # üß† Entrenamiento del VAE (versi√≥n estable) - DEPRECATED\n",
        "# # ============================================================\n",
        "\n",
        "# # obtenemos un batch de muestra para el callback\n",
        "# sample_batch = next(iter(train_ds_no_labels))\n",
        "\n",
        "# vae = VAE(encoder, decoder)\n",
        "# vae.compile(optimizer=tf.keras.optimizers.Adam(), loss=vae_total_loss)\n",
        "\n",
        "# vae.fit(\n",
        "#     train_ds_no_labels,\n",
        "#     validation_data=val_ds_no_labels,\n",
        "#     epochs=EPOCHS,\n",
        "#     steps_per_epoch=50,\n",
        "#     validation_steps=10,\n",
        "#     callbacks=[VisualCallback(sample_batch)],\n",
        "#     verbose=1\n",
        "# )\n",
        "\n",
        "\n",
        "# # ============================================================\n",
        "# # üíæ Guardado de modelos entrenados - DEPRECATED\n",
        "# # ============================================================\n",
        "\n",
        "# decoder.save(\"/content/drive/MyDrive/maquina-de-contrapropaganda/models/decoder_solo.keras\")\n",
        "# encoder.save(\"/content/drive/MyDrive/maquina-de-contrapropaganda/models/encoder_solo.keras\")\n",
        "# vae.save(\"/content/drive/MyDrive/maquina-de-contrapropaganda/models/vae_completo.keras\")\n",
        "\n",
        "# print(\"‚úÖ Modelos guardados correctamente en Drive.\")"
      ],
      "metadata": {
        "id": "L_CNHl2ElcHu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# ‚öôÔ∏è Funci√≥n para preparar el Dataset por Letra (GRIS)\n",
        "# ============================================================\n",
        "\n",
        "def prepare_dataset_for_letter(base_ds, target_label_name):\n",
        "    \"\"\"\n",
        "    Filtra, preprocesa y prepara el dataset para entrenar una sola letra en GRIS.\n",
        "    \"\"\"\n",
        "    global IMG_SIZE\n",
        "    BATCH_SIZE = 32\n",
        "    AUTOTUNE = tf.data.AUTOTUNE\n",
        "\n",
        "    target_label_int = builder.info.features[\"label\"].str2int(target_label_name)\n",
        "\n",
        "    # 1. Filtrar el dataset base\n",
        "    ds_filtered = base_ds.filter(lambda x, y: tf.equal(y, target_label_int)).map(lambda x, y: x)\n",
        "\n",
        "    def ensure_valid_image(img):\n",
        "        # Normalizar a [0, 1]\n",
        "        img = tf.image.convert_image_dtype(img, tf.float32)\n",
        "        # üö® CAMBIO CLAVE: Conversi√≥n a escala de grises\n",
        "        img = tf.image.rgb_to_grayscale(img)\n",
        "        img = tf.image.resize(img, [IMG_SIZE, IMG_SIZE])\n",
        "        # Aseguramos la forma: [64, 64, 1] (UN canal)\n",
        "        return tf.ensure_shape(img, [IMG_SIZE, IMG_SIZE, 1])\n",
        "\n",
        "    ds_final = (\n",
        "        ds_filtered\n",
        "        .map(ensure_valid_image, num_parallel_calls=AUTOTUNE)\n",
        "        .shuffle(512)\n",
        "        .batch(BATCH_SIZE)\n",
        "        .repeat()\n",
        "        .prefetch(AUTOTUNE)\n",
        "    )\n",
        "\n",
        "    return ds_final\n",
        "\n",
        "print(\"‚úÖ Funci√≥n prepare_dataset_for_letter actualizada a GRIS (1 canal).\")"
      ],
      "metadata": {
        "id": "ixmn0maPjSSU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3663a9aa"
      },
      "source": [
        "# ============================================================\n",
        "# üß† Definici√≥n del Encoder (versi√≥n estable GRIS)\n",
        "# ============================================================\n",
        "\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import tensorflow as tf\n",
        "# Se asumen LATENT_DIM=20, IMG_SIZE=64\n",
        "\n",
        "def sampling(args):\n",
        "    z_mean, z_log_var = args\n",
        "    batch = tf.shape(z_mean)[0]\n",
        "    dim = tf.shape(z_mean)[1]\n",
        "    epsilon = tf.random.normal(shape=(batch, dim))\n",
        "    return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
        "\n",
        "def make_encoder_model():\n",
        "    # üö® CAMBIO CLAVE: Input shape es IMG_SIZE x IMG_SIZE x 1 (GRIS)\n",
        "    encoder_inputs = keras.Input(shape=(IMG_SIZE, IMG_SIZE, 1))\n",
        "\n",
        "    # Resto de capas Conv2D... (la cantidad de filtros se mantiene igual)\n",
        "    x = layers.Conv2D(32, 3, activation=\"relu\", strides=2, padding=\"same\")(encoder_inputs)\n",
        "    x = layers.Conv2D(64, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\n",
        "    x = layers.Conv2D(64, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\n",
        "\n",
        "    x = layers.Flatten()(x)\n",
        "    x = layers.Dense(256, activation=\"relu\")(x)\n",
        "\n",
        "    z_mean = layers.Dense(LATENT_DIM, name=\"z_mean\")(x)\n",
        "    z_log_var = layers.Dense(LATENT_DIM, name=\"z_log_var\")(x)\n",
        "    z = layers.Lambda(sampling, name=\"z\")([z_mean, z_log_var])\n",
        "\n",
        "    encoder = keras.Model(encoder_inputs, [z_mean, z_log_var, z], name=\"encoder\")\n",
        "    return encoder\n",
        "\n",
        "print(\"‚úÖ Funci√≥n make_encoder_model actualizada a GRIS (1 canal de entrada).\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "af82f471"
      },
      "source": [
        "# ============================================================\n",
        "# üß† Definici√≥n del Decoder (versi√≥n estable GRIS)\n",
        "# ============================================================\n",
        "\n",
        "def make_decoder_model():\n",
        "    # Decoder network\n",
        "    latent_inputs = keras.Input(shape=(LATENT_DIM,))\n",
        "    x = layers.Dense(8 * 8 * 64, activation=\"relu\")(latent_inputs)\n",
        "    x = layers.Reshape((8, 8, 64))(x)\n",
        "    x = layers.Conv2DTranspose(64, 3, activation=\"relu\", strides=2, padding=\"same\")(x) # 16x16\n",
        "    x = layers.Conv2DTranspose(32, 3, activation=\"relu\", strides=2, padding=\"same\")(x) # 32x32\n",
        "\n",
        "    # üö® CAMBIO CLAVE: Capa final con 1 canal de salida\n",
        "    x = layers.Conv2DTranspose(1, 3, activation=\"sigmoid\", strides=2, padding=\"same\")(x) # 64x64x1\n",
        "\n",
        "    decoder_outputs = x\n",
        "    decoder = keras.Model(latent_inputs, decoder_outputs, name=\"decoder\")\n",
        "    return decoder\n",
        "\n",
        "print(\"‚úÖ Funci√≥n make_decoder_model actualizada a GRIS (1 canal de salida).\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# üß† Entrenamiento del VAE (Bucle por Letra)\n",
        "# ============================================================\n",
        "\n",
        "# Define las letras que quieres entrenar (A-Z)\n",
        "LETRAS_A_ENTRENAR = builder.info.features[\"label\"].names # Lista completa A-Z\n",
        "# Para pruebas, descomenta la l√≠nea de abajo:\n",
        "# LETRAS_A_ENTRENAR = ['A', 'B']\n",
        "\n",
        "EPOCHS_PER_LETTER = 50\n",
        "STEPS_PER_EPOCH = 50 # Un valor fijo. Ajusta seg√∫n el tama√±o de tu dataset.\n",
        "\n",
        "# Rutas de guardado (aseg√∫rate de que tu Drive est√© montado si usas esta ruta)\n",
        "MODEL_DIR = \"/content/drive/MyDrive/maquina-de-contrapropaganda/models_por_letra\"\n",
        "os.makedirs(MODEL_DIR, exist_ok=True)\n",
        "BASE_DATASET = ds\n",
        "\n",
        "for letter in LETRAS_A_ENTRENAR:\n",
        "    print(f\"\\n====================== INICIANDO ENTRENAMIENTO: {letter} ======================\")\n",
        "\n",
        "    # A. Preparar datasets espec√≠ficos para esta letra\n",
        "    # Usamos el dataset base 'ds' y lo filtramos por la letra.\n",
        "    train_ds_letter = prepare_dataset_for_letter(BASE_DATASET, letter)\n",
        "\n",
        "    # B. Obtener un batch de muestra para el callback de visualizaci√≥n\n",
        "    # Se obtienen 8 muestras de la letra que se va a entrenar.\n",
        "    sample_batch = next(iter(train_ds_letter.unbatch().take(8).batch(8)))\n",
        "\n",
        "    # C. RE-INICIALIZAR MODELOS (CLAVE para que aprenda desde cero cada letra)\n",
        "    # NOTA: Debes tener las funciones make_encoder_model() y make_decoder_model() definidas.\n",
        "    #\n",
        "    try:\n",
        "        encoder = make_encoder_model() # Funci√≥n que crea el modelo del Encoder\n",
        "        decoder = make_decoder_model() # Funci√≥n que crea el modelo del Decoder\n",
        "        vae = VAE(encoder, decoder)    # Clase VAE que combina ambos\n",
        "    except NameError:\n",
        "        print(\"‚ùå ERROR: make_encoder_model, make_decoder_model o VAE no est√°n definidas.\")\n",
        "        print(\"Por favor, comparte las celdas de la arquitectura del modelo.\")\n",
        "        continue # Salta a la siguiente letra\n",
        "\n",
        "    # D. Compilar y Entrenar\n",
        "    vae.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4), loss=vae_total_loss)\n",
        "\n",
        "    print(f\"üîπ Entrenamiento VAE para la letra '{letter}'...\")\n",
        "    vae.fit(\n",
        "        train_ds_letter,\n",
        "        epochs=EPOCHS_PER_LETTER,\n",
        "        steps_per_epoch=STEPS_PER_EPOCH,\n",
        "        callbacks=[VisualCallback(sample_batch, save_dir=f\"/content/outputs/{letter}\")],\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    # E. Guardar los modelos entrenados\n",
        "    decoder.save(os.path.join(MODEL_DIR, f\"decoder_{letter}.keras\"))\n",
        "    encoder.save(os.path.join(MODEL_DIR, f\"encoder_{letter}.keras\"))\n",
        "\n",
        "    print(f\"‚úÖ Modelos y salidas para la letra '{letter}' guardados en: {MODEL_DIR}\")"
      ],
      "metadata": {
        "id": "5hJXzmjzjZP4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6af32587"
      },
      "source": [
        "# ============================================================\n",
        "# ‚öôÔ∏è Generaci√≥n de Letras desde el Espacio Latente (CON GUARDADO PNG)\n",
        "# ============================================================\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "\n",
        "# --- Configuraciones Asumidas (las mismas) ---\n",
        "TEST_LETTER = 'C'\n",
        "MODEL_DIR = \"/content/drive/MyDrive/maquina-de-contrapropaganda/models_por_letra\"\n",
        "LATENT_DIM = 20\n",
        "N_SAMPLES = 10\n",
        "\n",
        "# --- Directorio de Salida para los PNGs ---\n",
        "OUTPUT_PNG_DIR = f\"/content/generated_letters/{TEST_LETTER}\"\n",
        "os.makedirs(OUTPUT_PNG_DIR, exist_ok=True)\n",
        "print(f\"Directorio de guardado: {OUTPUT_PNG_DIR}\")\n",
        "\n",
        "\n",
        "# 1. Cargar el modelo Decoder entrenado (usando el c√≥digo anterior)\n",
        "# ... (Aqu√≠ va tu c√≥digo de carga del decoder_model y load_weights) ...\n",
        "# Para que esta celda sea completa, asumo que tienes el c√≥digo de carga aqu√≠:\n",
        "try:\n",
        "    decoder_model = make_decoder_model()\n",
        "    decoder_path = os.path.join(MODEL_DIR, f\"decoder_{TEST_LETTER}.keras\")\n",
        "    decoder_model.load_weights(decoder_path)\n",
        "    print(f\"‚úÖ Decoder cargado exitosamente.\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå ERROR al cargar el Decoder. Aseg√∫rate de que el modelo exista. Error: {e}\")\n",
        "    raise\n",
        "\n",
        "\n",
        "# 2. Generar muestras aleatorias del Espacio Latente (Z)\n",
        "z_samples = tf.random.normal(shape=(N_SAMPLES, LATENT_DIM))\n",
        "\n",
        "# 3. Generar las im√°genes\n",
        "reconstructed_images = decoder_model.predict(z_samples)\n",
        "\n",
        "print(f\"‚úÖ Generaci√≥n completada. Guardando {N_SAMPLES} im√°genes como PNG...\")\n",
        "\n",
        "# 4. BUCLE DE GUARDADO DE PNG (¬°NUEVO!)\n",
        "for i in range(N_SAMPLES):\n",
        "    # Obtener la imagen, quitando la dimensi√≥n de canal 1\n",
        "    image_data = reconstructed_images[i, :, :, 0]\n",
        "\n",
        "    # Matplotlib puede guardar el array numpy directamente\n",
        "    plt.imsave(\n",
        "        os.path.join(OUTPUT_PNG_DIR, f\"letter_{TEST_LETTER}_{i:02d}.png\"),\n",
        "        image_data,\n",
        "        cmap='gray' # Es crucial especificar 'gray' para el formato gris\n",
        "    )\n",
        "\n",
        "# 5. Visualizaci√≥n (Opcional, pero √∫til para ver el resultado)\n",
        "fig = plt.figure(figsize=(12, 3))\n",
        "plt.suptitle(f\"Generaci√≥n y Guardado de {N_SAMPLES} PNGs\", fontsize=16)\n",
        "\n",
        "for i in range(N_SAMPLES):\n",
        "    ax = fig.add_subplot(1, N_SAMPLES, i + 1)\n",
        "    ax.imshow(reconstructed_images[i, :, :, 0], cmap='gray')\n",
        "    ax.axis(\"off\")\n",
        "\n",
        "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
        "plt.show()\n",
        "\n",
        "print(f\"‚úÖ Todas las im√°genes fueron guardadas en: {OUTPUT_PNG_DIR}\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}